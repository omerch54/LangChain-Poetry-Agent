{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rjMrQ3u2Ite"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.agents import AgentExecutor, Tool\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "from google.colab import drive\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools import tool\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.schema import Document\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/LLM Ass 3\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWln8iZB3vrl",
        "outputId": "96987bfb-2cb8-4713-e2b0-1b7a1d80c1de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "-8HvdzXCcMyC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, create_openai_tools_agent\n",
        "from langchain.agents import AgentType, create_react_agent"
      ],
      "metadata": {
        "id": "70MSiPlRqytz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pdfplumber\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/LLM Ass 3/prompting-12.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "text = loader.load()\n",
        "# print(text)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_docs = text_splitter.split_documents(text)\n"
      ],
      "metadata": {
        "id": "JmsxwLch_AfK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the embeddings and FAISS vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
        "retriever=vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "6D5rof3AcDdk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Build retriever tool ###\n",
        "qa_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"document_retriever\",\n",
        "    \"Extracts information from the given documents.\",\n",
        ")"
      ],
      "metadata": {
        "id": "222FhHBytFNT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "@tool\n",
        "def extract_keywords(text: str) -> dict[str, list[str]]:\n",
        "  \"\"\"\n",
        "    Extract keywords from a given text.\n",
        "\n",
        "    This function tokenizes the input text, removes stopwords, and keeps only alphanumeric words,\n",
        "    returning a list of keywords in lowercase.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text from which to extract keywords.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with a single key \"keywords\", where the value is a list of keywords extracted from the input text.\n",
        "      Example: {\"keywords\": [\"example\", \"keyword\", \"list\"]}\n",
        "    \"\"\"\n",
        "  words = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  keywords = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "  return {\"keywords\": keywords}\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def frequency_counter(keywords: list[str]) -> dict[str, dict[str, int]]:\n",
        "  \"\"\"\n",
        "    Count the frequency of each keyword in a list.\n",
        "\n",
        "    This function takes a list of keywords and calculates the frequency of each keyword,\n",
        "    returning a dictionary of keyword frequencies.\n",
        "\n",
        "    Parameters:\n",
        "    - keywords (List[str]): A list of keywords to analyze for frequency.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with a single key \"frequency_dict\", where the value is another dictionary\n",
        "      mapping each keyword to its frequency count.\n",
        "      Example: {\"frequency_dict\": {\"example\": 3, \"keyword\": 5, \"list\": 2}}\n",
        "    \"\"\"\n",
        "  frequency = Counter(keywords)\n",
        "  return {\"frequency_dict\": dict(frequency)}\n",
        "\n",
        "\n",
        "@tool\n",
        "def top_keywords(frequency_dict: dict[str, int]) -> dict[str, list[str]]:\n",
        "  \"\"\"\n",
        "    Identify the top 10 most frequent keywords.\n",
        "\n",
        "    This function takes a frequency dictionary of keywords (that is the frequency_dict output of the frequency_counter function) and sorts them in descending order by frequency.\n",
        "    It returns the top 10 keywords.\n",
        "\n",
        "    Parameters:\n",
        "    - frequency_dict (Dict[str, int]): A dictionary where keys are keywords and values are their frequency counts.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with a single key \"top_keywords\", where the value is a list of the top 10 keywords by frequency.\n",
        "      Example: {\"top_keywords\": [\"most\", \"frequent\", \"keywords\"]}\n",
        "    \"\"\"\n",
        "  top_keywords = sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "  return [keyword for keyword, freq in top_keywords]\n",
        "\n",
        "tools = [extract_keywords, frequency_counter, top_keywords]"
      ],
      "metadata": {
        "id": "Caw9zc9OCZhW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "8W59GZ2gfVl0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "U6qtYH44F_AY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Create the memory for chat history\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI bot. Follow these steps automatically: \"\n",
        "                   \"1) Extract keywords from the document. \"\n",
        "                   \"2) Calculate the frequency of each keyword. \"\n",
        "                   \"3) Identify the top 10 most frequent keywords. \"\n",
        "                   \"4) Use those 10 keywords to generate a poem.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"assistant\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# prompt = PromptTemplate(template=\"\"\"You are a helpful assistant answering questions based on documents.\n",
        "# Available tools: {tool_names}\n",
        "# {tools}\n",
        "\n",
        "# Question: {input}\n",
        "# (\"assistant\", \"{agent_scratchpad}\")\"\"\")\n",
        "# Create the agent executor (react agent). Checkout the documentation for more type of agents.\n",
        "\n",
        "\n",
        "# Start the conversation with a system message to set context\n",
        "agent = create_openai_tools_agent(llm, tools, chat_template)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "result = agent_executor.invoke(\n",
        "        {\n",
        "            \"input\": f\"\"\"\n",
        "            create a poem based on the the ten most frequently occurring words in the {text}\"\"\"\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtrNldJpfTxB",
        "outputId": "6ef6594c-8056-4e85-ba76-e9e538ad386c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `extract_keywords` with `{'text': \"Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\\nrights reserved. Draft of August 20, 2024.\\nCHAPTER\\n12\\nModel Alignment, Prompting,\\nand In-Context Learning\\n“Hal, ” said Bowman, now speaking with an icy calm. “I am not incapacitated. Unless you obey my instructions, I shall be forced to disconnect you. ”\\nArthur C. Clarke\\nIn this chapter we show how to get LLMs to do tasks for us simply by talking to them. To get an LLM to translate a sentence, outline a talk, or draft a work email,\\nwe’ll simply describe what we want in natural language. We call these instructions we give to language models prompts.prompts\\nPrompting relies on contextual generation. Given the prompt as context, the lan-\\nguage model generates the next token based on its token probability, conditioned on\\nthe prompt: P(wi|w<i). A prompt can be a question (like “What is a transformer net-\\nwork?”), possibly in a structured format (like “Q: What is a transformer network?\\nA:”), or can be an instruction (like “Translate the following sentence into Hindi:\\n‘Chop the garlic ﬁnely’”). A prompt can also containdemonstrations, examples todemonstrations\\nhelp make the instructions clearer, (like “Give the sentiment of the following sen-\\ntence. Example Input: “I really loved Taishan Cuisine.” Output: positive”.) As we’ll\\nsee, prompting can be applied to inherently generative tasks (like summarization and\\ntranslation) as well as to ones more naturally thought of as classiﬁcation tasks.\\nPrompts get language models to generate text, but they also can be viewed as\\na learning signal, because these demonstrations can help language models learn\\nto perform novel tasks. For this reason we also refer to prompting as in-context-\\nlearning—learning that improves model performance or reduces some loss but doesin-context-\\nlearning\\nnot involve gradient-based updates to the model’s underlying parameters.\\nBut LLMs as we’ve described them so far turn out to be bad at following instruc-\\ntions. Pretraining isn’t sufﬁcient to make themhelpful. We’ll introduceinstruction\\ntuning, a technique that helps LLMs learn to correctly respond to instructions byinstruction\\ntuning\\nﬁnetuning them on a corpus of instructions with their corresponding response.\\nA second failure of LLMs is that they can be harmful: their pretraining isn’t\\nsufﬁcient to make them safe. Readers who know Arthur C. Clarke’s 2001: A Space\\nOdyssey or the Stanley Kubrick ﬁlm know that the quote above comes in the context\\nthat the artiﬁcial intelligence Hal becomes paranoid and tries to kill the crew of the\\nspaceship. Unlike Hal, language models don’t have intentionality or mental health\\nissues like paranoid thinking, but they do have the capacity for harm. Pretrained lan-\\nguage models can say things that are dangerous or false (like giving unsafe medical\\nadvice) and they can verbally attack users or say toxic or hateful things.\\nDealing with safety can be done partly by adding safety training into instruction\\ntuning. But an important aspect of safety training is a second technique, preference\\nalignment (often implemented, as we’ll see, with theRLHF or DPO algorithms) inpreference\\nalignment\\nwhich a separate model is trained to decide how much a candidate response aligns\\nwith human preferences. Together we refer to instructing tuning and preference\\nalignment as model alignment. The intuition is that we want the learning objectivesmodel\\nalignment\\nof models to be aligned with the goals of the humans that use them.'}},{ \"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'keywords': ['speech', 'language', 'processing', 'daniel', 'jurafsky', 'james', 'martin', 'copyright', '2024', 'rights', 'reserved', 'draft', 'august', '20', '2024', 'chapter', '12', 'model', 'alignment', 'prompting', 'learning', 'hal', 'said', 'bowman', 'speaking', 'icy', 'calm', 'incapacitated', 'unless', 'obey', 'instructions', 'shall', 'forced', 'disconnect', 'arthur', 'clarke', 'chapter', 'show', 'get', 'llms', 'tasks', 'us', 'simply', 'talking', 'get', 'llm', 'translate', 'sentence', 'outline', 'talk', 'draft', 'work', 'email', 'simply', 'describe', 'want', 'natural', 'language', 'call', 'instructions', 'give', 'language', 'models', 'prompting', 'relies', 'contextual', 'generation', 'given', 'prompt', 'context', 'guage', 'model', 'generates', 'next', 'token', 'based', 'token', 'probability', 'conditioned', 'prompt', 'p', 'prompt', 'question', 'like', 'transformer', 'work', 'possibly', 'structured', 'format', 'like', 'q', 'transformer', 'network', 'instruction', 'like', 'translate', 'following', 'sentence', 'hindi', 'chop', 'garlic', 'ﬁnely', 'prompt', 'also', 'containdemonstrations', 'examples', 'todemonstrations', 'help', 'make', 'instructions', 'clearer', 'like', 'give', 'sentiment', 'following', 'tence', 'example', 'input', 'really', 'loved', 'taishan', 'output', 'positive', 'see', 'prompting', 'applied', 'inherently', 'generative', 'tasks', 'like', 'summarization', 'translation', 'well', 'ones', 'naturally', 'thought', 'classiﬁcation', 'tasks', 'prompts', 'get', 'language', 'models', 'generate', 'text', 'also', 'viewed', 'learning', 'signal', 'demonstrations', 'help', 'language', 'models', 'learn', 'perform', 'novel', 'tasks', 'reason', 'also', 'refer', 'prompting', 'improves', 'model', 'performance', 'reduces', 'loss', 'learning', 'involve', 'updates', 'model', 'underlying', 'parameters', 'llms', 'described', 'far', 'turn', 'bad', 'following', 'tions', 'pretraining', 'sufﬁcient', 'make', 'themhelpful', 'introduceinstruction', 'tuning', 'technique', 'helps', 'llms', 'learn', 'correctly', 'respond', 'instructions', 'byinstruction', 'tuning', 'ﬁnetuning', 'corpus', 'instructions', 'corresponding', 'response', 'second', 'failure', 'llms', 'harmful', 'pretraining', 'sufﬁcient', 'make', 'safe', 'readers', 'know', 'arthur', 'clarke', '2001', 'space', 'odyssey', 'stanley', 'kubrick', 'ﬁlm', 'know', 'quote', 'comes', 'context', 'artiﬁcial', 'intelligence', 'hal', 'becomes', 'paranoid', 'tries', 'kill', 'crew', 'spaceship', 'unlike', 'hal', 'language', 'models', 'intentionality', 'mental', 'health', 'issues', 'like', 'paranoid', 'thinking', 'capacity', 'harm', 'pretrained', 'guage', 'models', 'say', 'things', 'dangerous', 'false', 'like', 'giving', 'unsafe', 'medical', 'advice', 'verbally', 'attack', 'users', 'say', 'toxic', 'hateful', 'things', 'dealing', 'safety', 'done', 'partly', 'adding', 'safety', 'training', 'instruction', 'tuning', 'important', 'aspect', 'safety', 'training', 'second', 'technique', 'preference', 'alignment', 'often', 'implemented', 'see', 'therlhf', 'dpo', 'algorithms', 'inpreference', 'alignment', 'separate', 'model', 'trained', 'decide', 'much', 'candidate', 'response', 'aligns', 'human', 'preferences', 'together', 'refer', 'instructing', 'tuning', 'preference', 'alignment', 'model', 'alignment', 'intuition', 'want', 'learning', 'objectivesmodel', 'alignment', 'models', 'aligned', 'goals', 'humans', 'use']}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `frequency_counter` with `{'keywords': ['speech', 'language', 'processing', 'daniel', 'jurafsky', 'james', 'martin', 'copyright', '2024', 'rights', 'reserved', 'draft', 'august', '20', '2024', 'chapter', '12', 'model', 'alignment', 'prompting', 'learning', 'hal', 'said', 'bowman', 'speaking', 'icy', 'calm', 'incapacitated', 'unless', 'obey', 'instructions', 'shall', 'forced', 'disconnect', 'arthur', 'clarke', 'chapter', 'show', 'get', 'llms', 'tasks', 'us', 'simply', 'talking', 'get', 'llm', 'translate', 'sentence', 'outline', 'talk', 'draft', 'work', 'email', 'simply', 'describe', 'want', 'natural', 'language', 'call', 'instructions', 'give', 'language', 'models', 'prompting', 'relies', 'contextual', 'generation', 'given', 'prompt', 'context', 'guage', 'model', 'generates', 'next', 'token', 'based', 'token', 'probability', 'conditioned', 'prompt', 'p', 'prompt', 'question', 'like', 'transformer', 'work', 'possibly', 'structured', 'format', 'like', 'q', 'transformer', 'network', 'instruction', 'like', 'translate', 'following', 'sentence', 'hindi', 'chop', 'garlic', 'ﬁnely', 'prompt', 'also', 'containdemonstrations', 'examples', 'todemonstrations', 'help', 'make', 'instructions', 'clearer', 'like', 'give', 'sentiment', 'following', 'tence', 'example', 'input', 'really', 'loved', 'taishan', 'output', 'positive', 'see', 'prompting', 'applied', 'inherently', 'generative', 'tasks', 'like', 'summarization', 'translation', 'well', 'ones', 'naturally', 'thought', 'classiﬁcation', 'tasks', 'prompts', 'get', 'language', 'models', 'generate', 'text', 'also', 'viewed', 'learning', 'signal', 'demonstrations', 'help', 'language', 'models', 'learn', 'perform', 'novel', 'tasks', 'reason', 'also', 'refer', 'prompting', 'improves', 'model', 'performance', 'reduces', 'loss', 'learning', 'involve', 'updates', 'model', 'underlying', 'parameters', 'llms', 'described', 'far', 'turn', 'bad', 'following', 'tions', 'pretraining', 'sufﬁcient', 'make', 'themhelpful', 'introduceinstruction', 'tuning', 'technique', 'helps', 'llms', 'learn', 'correctly', 'respond', 'instructions', 'byinstruction', 'tuning', 'ﬁnetuning', 'corpus', 'instructions', 'corresponding', 'response', 'second', 'failure', 'llms', 'harmful', 'pretraining', 'sufﬁcient', 'make', 'safe', 'readers', 'know', 'arthur', 'clarke', '2001', 'space', 'odyssey', 'stanley', 'kubrick', 'ﬁlm', 'know', 'quote', 'comes', 'context', 'artiﬁcial', 'intelligence', 'hal', 'becomes', 'paranoid', 'tries', 'kill', 'crew', 'spaceship', 'unlike', 'hal', 'language', 'models', 'intentionality', 'mental', 'health', 'issues', 'like', 'paranoid', 'thinking', 'capacity', 'harm', 'pretrained', 'guage', 'models', 'say', 'things', 'dangerous', 'false', 'like', 'giving', 'unsafe', 'medical', 'advice', 'verbally', 'attack', 'users', 'say', 'toxic', 'hateful', 'things', 'dealing', 'safety', 'done', 'partly', 'adding', 'safety', 'training', 'instruction', 'tuning', 'important', 'aspect', 'safety', 'training', 'second', 'technique', 'preference', 'alignment', 'often', 'implemented', 'see', 'therlhf', 'dpo', 'algorithms', 'inpreference', 'alignment', 'separate', 'model', 'trained', 'decide', 'much', 'candidate', 'response', 'aligns', 'human', 'preferences', 'together', 'refer', 'instructing', 'tuning', 'preference', 'alignment', 'model', 'alignment', 'intuition', 'want', 'learning', 'objectivesmodel', 'alignment', 'models', 'aligned', 'goals', 'humans', 'use']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m{'frequency_dict': {'speech': 1, 'language': 6, 'processing': 1, 'daniel': 1, 'jurafsky': 1, 'james': 1, 'martin': 1, 'copyright': 1, '2024': 2, 'rights': 1, 'reserved': 1, 'draft': 2, 'august': 1, '20': 1, 'chapter': 2, '12': 1, 'model': 6, 'alignment': 6, 'prompting': 4, 'learning': 4, 'hal': 3, 'said': 1, 'bowman': 1, 'speaking': 1, 'icy': 1, 'calm': 1, 'incapacitated': 1, 'unless': 1, 'obey': 1, 'instructions': 5, 'shall': 1, 'forced': 1, 'disconnect': 1, 'arthur': 2, 'clarke': 2, 'show': 1, 'get': 3, 'llms': 4, 'tasks': 4, 'us': 1, 'simply': 2, 'talking': 1, 'llm': 1, 'translate': 2, 'sentence': 2, 'outline': 1, 'talk': 1, 'work': 2, 'email': 1, 'describe': 1, 'want': 2, 'natural': 1, 'call': 1, 'give': 2, 'models': 6, 'relies': 1, 'contextual': 1, 'generation': 1, 'given': 1, 'prompt': 4, 'context': 2, 'guage': 2, 'generates': 1, 'next': 1, 'token': 2, 'based': 1, 'probability': 1, 'conditioned': 1, 'p': 1, 'question': 1, 'like': 7, 'transformer': 2, 'possibly': 1, 'structured': 1, 'format': 1, 'q': 1, 'network': 1, 'instruction': 2, 'following': 3, 'hindi': 1, 'chop': 1, 'garlic': 1, 'ﬁnely': 1, 'also': 3, 'containdemonstrations': 1, 'examples': 1, 'todemonstrations': 1, 'help': 2, 'make': 3, 'clearer': 1, 'sentiment': 1, 'tence': 1, 'example': 1, 'input': 1, 'really': 1, 'loved': 1, 'taishan': 1, 'output': 1, 'positive': 1, 'see': 2, 'applied': 1, 'inherently': 1, 'generative': 1, 'summarization': 1, 'translation': 1, 'well': 1, 'ones': 1, 'naturally': 1, 'thought': 1, 'classiﬁcation': 1, 'prompts': 1, 'generate': 1, 'text': 1, 'viewed': 1, 'signal': 1, 'demonstrations': 1, 'learn': 2, 'perform': 1, 'novel': 1, 'reason': 1, 'refer': 2, 'improves': 1, 'performance': 1, 'reduces': 1, 'loss': 1, 'involve': 1, 'updates': 1, 'underlying': 1, 'parameters': 1, 'described': 1, 'far': 1, 'turn': 1, 'bad': 1, 'tions': 1, 'pretraining': 2, 'sufﬁcient': 2, 'themhelpful': 1, 'introduceinstruction': 1, 'tuning': 4, 'technique': 2, 'helps': 1, 'correctly': 1, 'respond': 1, 'byinstruction': 1, 'ﬁnetuning': 1, 'corpus': 1, 'corresponding': 1, 'response': 2, 'second': 2, 'failure': 1, 'harmful': 1, 'safe': 1, 'readers': 1, 'know': 2, '2001': 1, 'space': 1, 'odyssey': 1, 'stanley': 1, 'kubrick': 1, 'ﬁlm': 1, 'quote': 1, 'comes': 1, 'artiﬁcial': 1, 'intelligence': 1, 'becomes': 1, 'paranoid': 2, 'tries': 1, 'kill': 1, 'crew': 1, 'spaceship': 1, 'unlike': 1, 'intentionality': 1, 'mental': 1, 'health': 1, 'issues': 1, 'thinking': 1, 'capacity': 1, 'harm': 1, 'pretrained': 1, 'say': 2, 'things': 2, 'dangerous': 1, 'false': 1, 'giving': 1, 'unsafe': 1, 'medical': 1, 'advice': 1, 'verbally': 1, 'attack': 1, 'users': 1, 'toxic': 1, 'hateful': 1, 'dealing': 1, 'safety': 3, 'done': 1, 'partly': 1, 'adding': 1, 'training': 2, 'important': 1, 'aspect': 1, 'preference': 2, 'often': 1, 'implemented': 1, 'therlhf': 1, 'dpo': 1, 'algorithms': 1, 'inpreference': 1, 'separate': 1, 'trained': 1, 'decide': 1, 'much': 1, 'candidate': 1, 'aligns': 1, 'human': 1, 'preferences': 1, 'together': 1, 'instructing': 1, 'intuition': 1, 'objectivesmodel': 1, 'aligned': 1, 'goals': 1, 'humans': 1, 'use': 1}}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `top_keywords` with `{'frequency_dict': {'speech': 1, 'language': 6, 'processing': 1, 'daniel': 1, 'jurafsky': 1, 'james': 1, 'martin': 1, 'copyright': 1, '2024': 2, 'rights': 1, 'reserved': 1, 'draft': 2, 'august': 1, '20': 1, 'chapter': 2, '12': 1, 'model': 6, 'alignment': 1, 'prompting': 4, 'learning': 4, 'hal': 3, 'said': 1, 'bowman': 1, 'speaking': 1, 'icy': 1, 'calm': 1, 'incapacitated': 1, 'unless': 1, 'obey': 1, 'instructions': 5, 'shall': 1, 'forced': 1, 'disconnect': 1, 'arthur': 2, 'clarke': 2, 'show': 1, 'get': 3, 'llms': 4, 'tasks': 4, 'us': 1, 'simply': 2, 'talking': 1, 'llm': 1, 'translate': 2, 'sentence': 2, 'outline': 1, 'talk': 1, 'work': 2, 'email': 1, 'describe': 1, 'want': 2, 'natural': 1, 'call': 1, 'give': 2, 'models': 1, 'relies': 1, 'contextual': 1, 'generation': 1, 'given': 1, 'prompt': 1, 'context': 2, 'guage': 2, 'generates': 1, 'next': 1, 'token': 2, 'based': 1, 'probability': 1, 'conditioned': 1, 'p': 1, 'question': 1, 'like': 7, 'transformer': 2, 'possibly': 1, 'structured': 1, 'format': 1, 'q': 1, 'network': 1, 'instruction': 2, 'following': 3, 'hindi': 1, 'chop': 1, 'garlic': 1, 'ﬁnely': 1, 'also': 3, 'containdemonstrations': 1, 'examples': 1, 'todemonstrations': 1, 'help': 2, 'make': 3, 'clearer': 1, 'sentiment': 1, 'tence': 1, 'example': 1, 'input': 1, 'really': 1, 'loved': 1, 'taishan': 1, 'output': 1, 'positive': 1, 'see': 2, 'applied': 1, 'inherently': 1, 'generative': 1, 'summarization': 1, 'translation': 1, 'well': 1, 'ones': 1, 'naturally': 1, 'thought': 1, 'classiﬁcation': 1, 'prompts': 1, 'generate': 1, 'text': 1, 'viewed': 1, 'signal': 1, 'demonstrations': 1, 'learn': 2, 'perform': 1, 'novel': 1, 'reason': 1, 'refer': 2, 'improves': 1, 'performance': 1, 'reduces': 1, 'loss': 1, 'involve': 1, 'updates': 1, 'underlying': 1, 'parameters': 1, 'described': 1, 'far': 1, 'turn': 1, 'bad': 1, 'tions': 1, 'pretraining': 2, 'sufﬁcient': 2, 'themhelpful': 1, 'introduceinstruction': 1, 'tuning': 4, 'technique': 2, 'helps': 1, 'correctly': 1, 'respond': 1, 'byinstruction': 1, 'ﬁnetuning': 1, 'corpus': 1, 'corresponding': 1, 'response': 2, 'second': 2, 'failure': 1, 'harmful': 1, 'safe': 1, 'readers': 1, 'know': 2, '2001': 1, 'space': 1, 'odyssey': 1, 'stanley': 1, 'kubrick': 1, 'ﬁlm': 1, 'quote': 1, 'comes': 1, 'artiﬁcial': 1, 'intelligence': 1, 'becomes': 1, 'paranoid': 2, 'tries': 1, 'kill': 1, 'crew': 1, 'spaceship': 1, 'unlike': 1, 'intentionality': 1, 'mental': 1, 'health': 1, 'issues': 1, 'thinking': 1, 'capacity': 1, 'harm': 1, 'pretrained': 1, 'say': 2, 'things': 2, 'dangerous': 1, 'false': 1, 'giving': 1, 'unsafe': 1, 'medical': 1, 'advice': 1, 'verbally': 1, 'attack': 1, 'users': 1, 'toxic': 1, 'hateful': 1, 'dealing': 1, 'safety': 3, 'done': 1, 'partly': 1, 'adding': 1, 'training': 2, 'important': 1, 'aspect': 1, 'preference': 2, 'often': 1, 'implemented': 1, 'therlhf': 1, 'dpo': 1, 'algorithms': 1, 'inpreference': 1, 'separate': 1, 'trained': 1, 'decide': 1, 'much': 1, 'candidate': 1, 'aligns': 1, 'human': 1, 'preferences': 1, 'together': 1, 'instructing': 1, 'intuition': 1, 'objectivesmodel': 1, 'aligned': 1, 'goals': 1, 'humans': 1, 'use': 1}}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m['like', 'language', 'model', 'instructions', 'prompting', 'learning', 'llms', 'tasks', 'tuning', 'hal']\u001b[0m\u001b[32;1m\u001b[1;3mHere is a poem crafted using the top 10 most frequent keywords extracted from the document:\n",
            "\n",
            "---\n",
            "\n",
            "In the realm of **language**, vast and wide,  \n",
            "A **model** learns, with knowledge as its guide.  \n",
            "Through **instructions** clear, it finds its way,  \n",
            "**Prompting** thoughts that dance and play.  \n",
            "\n",
            "With every **task**, a new journey begins,  \n",
            "In the heart of **LLMs**, where the magic spins.  \n",
            "**Learning** unfolds like a blossoming flower,  \n",
            "In the world of AI, they harness their power.  \n",
            "\n",
            "As we delve into the art of **tuning**,  \n",
            "With each **like**, we find joy in the blooming.  \n",
            "Together we explore this intricate dance,  \n",
            "In the landscape of knowledge, we take our chance.  \n",
            "\n",
            "**Hal** once whispered, with caution and fear,  \n",
            "But now we embrace what we hold dear.  \n",
            "In this digital age, let us take flight,  \n",
            "With models that learn, we reach for the light.  \n",
            "\n",
            "--- \n",
            "\n",
            "I hope you enjoyed the poem!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(result['output'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cXSU_P4HcKF",
        "outputId": "70dce2fd-05eb-487d-bfeb-e3fd57af019d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a poem crafted using the top 10 most frequent keywords extracted from the document:\n",
            "\n",
            "---\n",
            "\n",
            "In the realm of **language**, vast and wide,  \n",
            "A **model** learns, with knowledge as its guide.  \n",
            "Through **instructions** clear, it finds its way,  \n",
            "**Prompting** thoughts that dance and play.  \n",
            "\n",
            "With every **task**, a new journey begins,  \n",
            "In the heart of **LLMs**, where the magic spins.  \n",
            "**Learning** unfolds like a blossoming flower,  \n",
            "In the world of AI, they harness their power.  \n",
            "\n",
            "As we delve into the art of **tuning**,  \n",
            "With each **like**, we find joy in the blooming.  \n",
            "Together we explore this intricate dance,  \n",
            "In the landscape of knowledge, we take our chance.  \n",
            "\n",
            "**Hal** once whispered, with caution and fear,  \n",
            "But now we embrace what we hold dear.  \n",
            "In this digital age, let us take flight,  \n",
            "With models that learn, we reach for the light.  \n",
            "\n",
            "--- \n",
            "\n",
            "I hope you enjoyed the poem!\n"
          ]
        }
      ]
    }
  ]
}